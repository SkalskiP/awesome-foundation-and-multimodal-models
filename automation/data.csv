title, date, paper, code, model_type
"GPT-4V(ision)", 25-09-2023,https://cdn.openai.com/papers/GPTV_System_Card.pdf,,multimodal
"Kosmos-2: Grounding Multimodal Large Language Models to the World",26-07-2023,2306.14824,https://github.com/microsoft/unilm, multimodal
"LLaVA: Large Language and Vision Assistant",17-04-2023,2304.08485,https://github.com/haotian-liu/LLaVA, multimodal
"Segment Anything",05-04-2023,2304.02643,https://github.com/facebookresearch/segment-anything, foundation
"Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection",09-03-2023,2303.05499,https://github.com/IDEA-Research/GroundingDINO, foundation
"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",30-01-2023,2301.12597,https://github.com/salesforce/LAVIS, foundation
"OWL-ST: Scaling Open-Vocabulary Object Detection",16-01-2023,2306.09683,"", foundation
"Flamingo: a Visual Language Model for Few-Shot Learning (Open Source implementation is 'Open Flamingo')",29-04-2022,2204.14198,https://github.com/mlfoundations/open_flamingo, multimodal
"OWL-ViT: Simple Open-Vocabulary Object Detection with Vision Transformers",12-05-2022,2205.06230,https://github.com/google-research/scenic, foundation
"CLIP: Learning Transferable Visual Models From Natural Language Supervision",26-02-2021,2103.00020,https://github.com/openai/CLIP, foundation