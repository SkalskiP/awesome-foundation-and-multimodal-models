title," authors"," date"," paper"," code"," huggingface","huggingface_models"," colab"," modalities"," tasks"
"YOLO-World: Real-Time Open-Vocabulary Object Detection","Tianheng Cheng, Lin Song, Yixiao Ge, Wenyu Liu, Xinggang Wang, Ying Shan",30-01-2024,2401.17270,https://github.com/AILab-CVC/YOLO-World,https://huggingface.co/spaces/SkalskiP/YOLO-World,,https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/zero-shot-object-detection-with-yolo-world.ipynb,👁️ + 💬,"Zero-Shot Object Detection"
"Depth Anything","Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, Hengshuang Zhao",19-01-2024,2401.10891,https://github.com/LiheYoung/Depth-Anything,https://huggingface.co/spaces/LiheYoung/Depth-Anything,https://huggingface.co/LiheYoung/depth_anything_vitl14,https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/Depth%20Anything/Predicting_depth_in_an_image_with_Depth_Anything.ipynb,👁️,"Depth Estimation"
EfficientSAM: Leveraged Masked Image Pretraining for Efficient Segment Anything,"Yunyang Xiong, Bala Varadarajan, Lemeng Wu, Xiaoyu Xiang, Fanyi Xiao, Chenchen Zhu, Xiaoliang Dai, Dilin Wang, Fei Sun, Forrest Iandola, Raghuraman Krishnamoorthi, Vikas Chandra",01-12-2023,2312.00863,https://github.com/yformer/EfficientSAM,https://huggingface.co/spaces/SkalskiP/EfficientSAM,https://huggingface.co/merve/EfficientSAM,,👁️,"Zero-Shot Object Segmentation"
CogVLM: Visual Expert for Pretrained Language Models,"Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, Jie Tang",06-11-2023,2311.03079,https://github.com/THUDM/CogVLM,https://huggingface.co/spaces/lykeven/CogVLM,https://huggingface.co/THUDM/CogVLM,,👁️ + 💬,"Image Captioning, VQA"
Qwen-VL-Plus / Max,"Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, Jingren Zhou",28-11-2023,2308.12966,https://github.com/QwenLM/Qwen-VL#qwen-vl-plus,https://huggingface.co/spaces/Qwen/Qwen-VL-Plus,https://huggingface.co/Qwen/Qwen-VL,,👁️ + 💬,"Image Captioning, VQA, Zero-Shot Object Detection"
Fuyu-8B: A Multimodal Architecture for AI Agents,"Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, Sağnak Taşırlar",17-10-2023,,,https://huggingface.co/spaces/adept/fuyu-8b-demo,https://huggingface.co/adept/fuyu-8b,,👁️ + 💬,"Image Classification, Image Captioning, VQA, Find Text in Image"
Ferret: Refer and Ground Anything Anywhere at Any Granularity,"Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, Yinfei Yang",11-10-2023,2310.07704,https://github.com/apple/ml-ferret,,,,👁️ + 💬,"Image Captioning, VQA, Phrase Grounding, Object Detection"
MetaCLIP: Demystifying CLIP Data,"Hu Xu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Russell Howes, Vasu Sharma, Shang-Wen Li, Gargi Ghosh, Luke Zettlemoyer, Christoph Feichtenhofer",28-09-2023,2309.16671,https://github.com/facebookresearch/MetaCLIP,https://huggingface.co/spaces/SkalskiP/SAM_and_MetaCLIP,https://huggingface.co/facebook/metaclip-b32-400m,https://colab.research.google.com/drive/1V0Rv1QQJkcolTjiwJuRsqWycROvYjOwg?usp=sharing,👁️ + 💬,"Zero-Shot Classification"
"Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond","Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, Jingren Zhou",24-09-2023,2308.12966,https://github.com/QwenLM/Qwen-VL,https://huggingface.co/spaces/Qwen/Qwen-VL-Max,https://huggingface.co/Qwen/Qwen-VL,,👁️ + 💬,"Image Captioning, VQA"
"SigLIP: Sigmoid Loss for Language Image Pre-Training","Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, Lucas Beyer",27-08-2023,2303.15343,https://github.com/google-research/big_vision,https://huggingface.co/spaces/merve/compare_clip_siglip,https://huggingface.co/openai/clip-vit-base-patch16,https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/SigLIP/Inference_with_(multilingual)_SigLIP%2C_a_better_CLIP_model.ipynb,👁️💬,"Zero-Shot Image Classification"
"Nougat: Neural Optical Understanding for Academic Documents","Lukas Blecher, Guillem Cucurull, Thomas Scialom, Robert Stojnic",25-08-2023,2308.13418,https://github.com/facebookresearch/nougat,https://huggingface.co/spaces/hf-vision/nougat-transformers,https://huggingface.co/facebook/nougat-small,https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/Nougat/Inference_with_Nougat_to_read_scientific_PDFs.ipynb,👁️💬,"Visual Question Answering"
AudioLDM 2: Learning Holistic Audio Generation with Self-supervised Pretraining,"Haohe Liu, Qiao Tian, Yi Yuan, Xubo Liu, Xinhao Mei, Qiuqiang Kong, Yuping Wang, Wenwu Wang, Yuxuan Wang, Mark D. Plumbley",10-08-2023,2308.05734,https://github.com/haoheliu/AudioLDM2,https://huggingface.co/spaces/haoheliu/AudioLDM_48K_Text-to-HiFiAudio_Generation,https://huggingface.co/spaces/haoheliu/audioldm2-text2audio-text2music,,💬️ + 🎧,"Text-to-Audio, Text-to-Speech"
OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models,"Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, Ludwig Schmidt",02-08-2023,2308.01390,https://github.com/mlfoundations/open_flamingo,https://huggingface.co/spaces/openflamingo/OpenFlamingo,https://huggingface.co/openflamingo/OpenFlamingo-9B-vitl-mpt7b,,👁️ + 💬,"Image Classification, Image Captioning, VQA"
Kosmos-2: Grounding Multimodal Large Language Models to the World,"Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, Furu Wei",26-07-2023,2306.14824,https://github.com/microsoft/unilm/tree/master/kosmos-2,https://huggingface.co/spaces/ydshieh/Kosmos-2,https://huggingface.co/microsoft/kosmos-2-patch14-224,,👁️ + 💬,"Image Captioning, VQA, Phrase Grounding"
"OWLv2: Scaling Open-Vocabulary Object Detection","Matthias Minderer, Alexey Gritsenko, Neil Houlsby",17-06-2023,2306.09683,https://github.com/google-research/scenic/tree/main/scenic/projects/owl_vit,https://huggingface.co/spaces/merve/owlv2,https://huggingface.co/google/owlv2-base-patch16-ensemble,https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/OWLv2/Zero_and_one_shot_object_detection_with_OWLv2.ipynb,👁️,"Zero-Shot Object Detection"
ImageBind: One Embedding Space To Bind Them All,"Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, Ishan Misra",09-05-2023,2305.05665,https://github.com/facebookresearch/ImageBind,https://huggingface.co/spaces/JustinLin610/ImageBind_zeroshot_demo,,,👁️ + 💬 + 🎧,
LLaVA: Large Language and Vision Assistant,"Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee",17-04-2023,2304.08485,https://github.com/haotian-liu/LLaVA,https://huggingface.co/spaces/badayvedat/LLaVA,https://huggingface.co/liuhaotian/llava-v1.6-34b,,👁️ + 💬,"Vision Language Modeling"
Segment Anything,"Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, Ross Girshick",05-04-2023,2304.02643,https://github.com/facebookresearch/segment-anything,https://huggingface.co/spaces/radames/candle-segment-anything-wasm,https://huggingface.co/facebook/sam-vit-base,https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-segment-anything-with-sam.ipynb,👁️,"Zero-Shot Object Segmentation"
Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection,"Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, Lei Zhang",09-03-2023,2303.05499,https://github.com/IDEA-Research/GroundingDINO,https://huggingface.co/spaces/ShilongLiu/Grounding_DINO_demo,https://huggingface.co/spaces/merve/Grounding_DINO_demo,https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/zero-shot-object-detection-with-grounding-dino.ipynb,👁️ + 💬,"Phrase Grounding, Zero-Shot Object Detection"
BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models,"Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi",30-01-2023,2301.12597,https://github.com/salesforce/LAVIS/tree/main/projects/blip2,https://huggingface.co/spaces/merve/BLIP2-with-transformers,https://huggingface.co/Salesforce/blip2-opt-6.7b,https://colab.research.google.com/github/salesforce/LAVIS/blob/main/examples/blip2_instructed_generation.ipynb,👁️ + 💬,"Image Captioning, Visual Question Answering"
Whisper: Robust Speech Recognition via Large-Scale Weak Supervision,"Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, Ilya Sutskever",06-12-2022,2212.04356,https://github.com/openai/whisper,https://huggingface.co/spaces/openai/whisper,https://huggingface.co/openai/whisper-large-v3,https://colab.research.google.com/github/openai/whisper/blob/master/notebooks/LibriSpeech.ipynb,💬️ + 🎧,"Speech-to-Text"
OWL-ViT: Simple Open-Vocabulary Object Detection with Vision Transformers,"Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, Neil Houlsby",12-05-2022,2205.06230,https://github.com/google-research/scenic/tree/main/scenic/projects/owl_vit,https://huggingface.co/spaces/adirik/OWL-ViT,https://huggingface.co/google/owlvit-base-patch32,,👁️ + 💬,"Zero-Shot Object Detection"
CLIP: Learning Transferable Visual Models From Natural Language Supervision,"Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever",26-02-2021,2103.00020,https://github.com/openai/CLIP,,https://huggingface.co/openai/clip-vit-large-patch14,https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-use-openai-clip-classification.ipynb,👁️ + 💬,"Zero-Shot Classification"