<h1 align="center">awesome foundation and multimodal models</h1>

## ğŸ‘ï¸ + ğŸ’¬ + ğŸ§ = ğŸ¤–

**foundation model** - a pre-trained machine learning model that serves as a base for a wide range of downstream tasks. It captures general knowledge from a large dataset and can be fine-tuned to perform specific tasks more effectively.

**multimodal model** - a model that can process multiple modalities (e.g. text, image,
video, audio, etc.) at the same time.

## ğŸ—ï¸ papers

<!--- AUTOGENERATED_PAPERS_LIST -->
<!---
   WARNING: DO NOT EDIT THIS LIST MANUALLY. IT IS AUTOMATICALLY GENERATED.
   HEAD OVER TO CONTRIBUTING.MD FOR MORE DETAILS ON HOW TO MAKE CHANGES PROPERLY.
-->

### GPT-4 Vision
   

 "OpenAI researchers listed in [GPT-4V(ision) technical work and authors](https://openai.com/contributions/gpt-4v)"
- **Date:**  25-09-2023
- **Modalities:** ğŸ‘ï¸ + ğŸ’¬+ ğŸ§
- **Tasks:** VQA, Classification
    

### AudioLDM 2: Learning Holistic Audio Generation with Self-supervised Pretraining
[![arXiv](https://img.shields.io/badge/arXiv-2308.05734-b31b1b.svg)](https://arxiv.org/abs/2308.05734) [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/haoheliu/AudioLDM2) [![Gradio](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/haoheliu/AudioLDM_48K_Text-to-HiFiAudio_Generation) 

Haohe Liu, Qiao Tian, Yi Yuan, Xubo Liu, Xinhao Mei, Qiuqiang Kong, Yuping Wang, Wenwu Wang, Yuxuan Wang, Mark D. Plumbley
- **Date:** 10-08-2023
- **Modalities:** ğŸ’¬ï¸ + ğŸ§
- **Tasks:** Text-to-Audio, Text-to-Speech
    

### OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models
[![arXiv](https://img.shields.io/badge/arXiv-2308.01390-b31b1b.svg)](https://arxiv.org/abs/2308.01390) [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/mlfoundations/open_flamingo) [![Gradio](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/openflamingo/OpenFlamingo) 

Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, Ludwig Schmidt
- **Date:** 02-08-2023
- **Modalities:** ğŸ‘ï¸ + ğŸ’¬
- **Tasks:** Image Classification, Image Captioning, VQA
    

### Kosmos-2: Grounding Multimodal Large Language Models to the World
[![arXiv](https://img.shields.io/badge/arXiv-2306.14824-b31b1b.svg)](https://arxiv.org/abs/2306.14824) [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/microsoft/unilm/tree/master/kosmos-2) [![Gradio](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/ydshieh/Kosmos-2) 

Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, Furu Wei
- **Date:** 26-07-2023
- **Modalities:** ğŸ‘ï¸ + ğŸ’¬
- **Tasks:** Image Captioning, VQA, Phrase Grounding
    

### LLaVA: Large Language and Vision Assistant
[![arXiv](https://img.shields.io/badge/arXiv-2304.08485-b31b1b.svg)](https://arxiv.org/abs/2304.08485) [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/haotian-liu/LLaVA) [![Gradio](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/badayvedat/LLaVA) 

Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee
- **Date:** 17-04-2023
- **Modalities:** ğŸ‘ï¸ + ğŸ’¬
- **Tasks:** 
    

### ImageBind: One Embedding Space To Bind Them All
[![arXiv](https://img.shields.io/badge/arXiv-2305.05665-b31b1b.svg)](https://arxiv.org/abs/2305.05665) [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/facebookresearch/ImageBind)  

Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, Ishan Misra
- **Date:** 09-05-2023
- **Modalities:** ğŸ‘ï¸ + ğŸ’¬ + ğŸ§
- **Tasks:** 
    

### Segment Anything
[![arXiv](https://img.shields.io/badge/arXiv-2304.02643-b31b1b.svg)](https://arxiv.org/abs/2304.02643) [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/facebookresearch/segment-anything)  [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-segment-anything-with-sam.ipynb)

Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr DollÃ¡r, Ross Girshick
- **Date:** 05-04-2023
- **Modalities:** ğŸ‘ï¸
- **Tasks:** 
    

### Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection
[![arXiv](https://img.shields.io/badge/arXiv-2303.05499-b31b1b.svg)](https://arxiv.org/abs/2303.05499) [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/IDEA-Research/GroundingDINO) [![Gradio](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/ShilongLiu/Grounding_DINO_demo) [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/zero-shot-object-detection-with-grounding-dino.ipynb)

Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, Lei Zhang
- **Date:** 09-03-2023
- **Modalities:** ğŸ‘ï¸ + ğŸ’¬
- **Tasks:** 
    

### BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models
[![arXiv](https://img.shields.io/badge/arXiv-2301.12597-b31b1b.svg)](https://arxiv.org/abs/2301.12597) [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/salesforce/LAVIS/tree/main/projects/blip2) [![Gradio](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/Salesforce/BLIP2) [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/salesforce/LAVIS/blob/main/examples/blip2_instructed_generation.ipynb)

Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi
- **Date:** 30-01-2023
- **Modalities:** ğŸ‘ï¸ + ğŸ’¬
- **Tasks:** 
    

### OWL-ST: Scaling Open-Vocabulary Object Detection
[![arXiv](https://img.shields.io/badge/arXiv-2306.09683-b31b1b.svg)](https://arxiv.org/abs/2306.09683)  [![Gradio](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/merve/owlv2) 

Matthias Minderer, Alexey Gritsenko, Neil Houlsby
- **Date:** 16-01-2023
- **Modalities:** ğŸ‘ï¸ + ğŸ’¬
- **Tasks:** 
    

### Whisper: Robust Speech Recognition via Large-Scale Weak Supervision
[![arXiv](https://img.shields.io/badge/arXiv-2212.04356-b31b1b.svg)](https://arxiv.org/abs/2212.04356) [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/openai/whisper)  [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/openai/whisper/blob/master/notebooks/LibriSpeech.ipynb)

Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, Ilya Sutskever
- **Date:** 06-12-2022
- **Modalities:** ğŸ’¬ï¸ + ğŸ§
- **Tasks:** 
    

### OWL-ViT: Simple Open-Vocabulary Object Detection with Vision Transformers
[![arXiv](https://img.shields.io/badge/arXiv-2205.06230-b31b1b.svg)](https://arxiv.org/abs/2205.06230) [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/google-research/scenic/tree/main/scenic/projects/owl_vit) [![Gradio](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/adirik/OWL-ViT) 

Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, Neil Houlsby
- **Date:** 12-05-2022
- **Modalities:** ğŸ‘ï¸ + ğŸ’¬
- **Tasks:** 
    

### CLIP: Learning Transferable Visual Models From Natural Language Supervision
[![arXiv](https://img.shields.io/badge/arXiv-2103.00020-b31b1b.svg)](https://arxiv.org/abs/2103.00020) [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/openai/CLIP)  [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-use-openai-clip-classification.ipynb)

Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever
- **Date:** 26-02-2021
- **Modalities:** ğŸ‘ï¸ + ğŸ’¬
- **Tasks:** 
    
<!--- AUTOGENERATED_PAPERS_LIST -->

## ğŸ¦¸ contribution

We would love your help in making this repository even better! If you know of an
amazing paper that isn't listed here, or if you have any suggestions for improvement,
feel free to open an [issue](https://github.com/SkalskiP/awesome-foundation-and-multimodal-models/issues)
or submit a [pull request](https://github.com/SkalskiP/awesome-foundation-and-multimodal-models/pulls).
