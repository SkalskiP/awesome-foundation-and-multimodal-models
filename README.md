<h1 align="center">awesome foundation and multimodal models</h1>

## üëÅÔ∏è + üí¨ + üëÇüèª = ü§ñ

**foundation model** - a pre-trained machine learning model that serves as a base for a wide range of downstream tasks. It captures general knowledge from a large dataset and can be fine-tuned to perform specific tasks more effectively.

**multimodal model** - a model that can process multiple modalities (e.g. text, image,
video, audio, etc.) at the same time.

## üóûÔ∏è papers

<!--- AUTOGENERATED_COURSES_TABLE -->
<!---
   WARNING: DO NOT EDIT THIS TABLE MANUALLY. IT IS AUTOMATICALLY GENERATED.
   HEAD OVER TO CONTRIBUTING.MD FOR MORE DETAILS ON HOW TO MAKE CHANGES PROPERLY.
-->
| **title** | **date** | **repository / paper** | **model type** |
|:---------:|:--------:|:----------------------:|:--------------:|
| GPT-4V(ision) |  25-09-2023 |   [paper](https://cdn.openai.com/papers/GPTV_System_Card.pdf) | multimodal |
| Kosmos-2: Grounding Multimodal Large Language Models to the World | 26-07-2023 | [![GitHub](https://img.shields.io/github/stars/microsoft/unilm?style=social)](https://github.com/microsoft/unilm) [![arXiv](https://img.shields.io/badge/arXiv-2306.14824-b31b1b.svg)](https://arxiv.org/abs/2306.14824)  |  multimodal |
| LLaVA: Large Language and Vision Assistant | 17-04-2023 | [![GitHub](https://img.shields.io/github/stars/haotian-liu/LLaVA?style=social)](https://github.com/haotian-liu/LLaVA) [![arXiv](https://img.shields.io/badge/arXiv-2304.08485-b31b1b.svg)](https://arxiv.org/abs/2304.08485)  |  multimodal |
| Segment Anything | 05-04-2023 | [![GitHub](https://img.shields.io/github/stars/facebookresearch/segment-anything?style=social)](https://github.com/facebookresearch/segment-anything) [![arXiv](https://img.shields.io/badge/arXiv-2304.02643-b31b1b.svg)](https://arxiv.org/abs/2304.02643)  |  foundation |
| Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection | 09-03-2023 | [![GitHub](https://img.shields.io/github/stars/IDEA-Research/GroundingDINO?style=social)](https://github.com/IDEA-Research/GroundingDINO) [![arXiv](https://img.shields.io/badge/arXiv-2303.05499-b31b1b.svg)](https://arxiv.org/abs/2303.05499)  |  foundation |
| BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models | 30-01-2023 | [![GitHub](https://img.shields.io/github/stars/salesforce/LAVIS?style=social)](https://github.com/salesforce/LAVIS) [![arXiv](https://img.shields.io/badge/arXiv-2301.12597-b31b1b.svg)](https://arxiv.org/abs/2301.12597)  |  foundation |
| OWL-ST: Scaling Open-Vocabulary Object Detection | 16-01-2023 |  [![arXiv](https://img.shields.io/badge/arXiv-2306.09683-b31b1b.svg)](https://arxiv.org/abs/2306.09683)  |  foundation |
| Flamingo: a Visual Language Model for Few-Shot Learning (Open Source implementation is 'Open Flamingo') | 29-04-2022 | [![GitHub](https://img.shields.io/github/stars/mlfoundations/open_flamingo?style=social)](https://github.com/mlfoundations/open_flamingo) [![arXiv](https://img.shields.io/badge/arXiv-2204.14198-b31b1b.svg)](https://arxiv.org/abs/2204.14198)  |  multimodal |
| OWL-ViT: Simple Open-Vocabulary Object Detection with Vision Transformers | 12-05-2022 | [![GitHub](https://img.shields.io/github/stars/google-research/scenic?style=social)](https://github.com/google-research/scenic) [![arXiv](https://img.shields.io/badge/arXiv-2205.06230-b31b1b.svg)](https://arxiv.org/abs/2205.06230)  |  foundation |
| CLIP: Learning Transferable Visual Models From Natural Language Supervision | 26-02-2021 | [![GitHub](https://img.shields.io/github/stars/openai/CLIP?style=social)](https://github.com/openai/CLIP) [![arXiv](https://img.shields.io/badge/arXiv-2103.00020-b31b1b.svg)](https://arxiv.org/abs/2103.00020)  |  foundation |
<!--- AUTOGENERATED_COURSES_TABLE -->

## ü¶∏ contribution

We would love your help in making this repository even better! If you know of an
amazing paper that isn't listed here, or if you have any suggestions for improvement,
feel free to open an [issue](https://github.com/SkalskiP/awesome-foundation-and-multimodal-models/issues)
or submit a [pull request](https://github.com/SkalskiP/awesome-foundation-and-multimodal-models/pulls).
